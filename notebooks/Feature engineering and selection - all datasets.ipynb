{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature engineering and selection\n",
    "\n",
    "In this notebook the follow tasks will be performed\n",
    "\n",
    "1. Dataset loading\n",
    "2. Dataset split\n",
    "3. Continuous vs discrete variables\n",
    "4. Discrete variables reordering\n",
    "5. Continuous Target Transformation\n",
    "7. Removing Correlated variables\n",
    "8. Feature Scaling\n",
    "8. Feature selection by Lasso\n",
    "9. Saving processed dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/completion_rate.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>form_id</th>\n",
       "      <th>views</th>\n",
       "      <th>submissions</th>\n",
       "      <th>feat_01</th>\n",
       "      <th>feat_02</th>\n",
       "      <th>feat_03</th>\n",
       "      <th>feat_04</th>\n",
       "      <th>feat_05</th>\n",
       "      <th>feat_06</th>\n",
       "      <th>feat_07</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_38</th>\n",
       "      <th>feat_39</th>\n",
       "      <th>feat_40</th>\n",
       "      <th>feat_41</th>\n",
       "      <th>feat_42</th>\n",
       "      <th>feat_43</th>\n",
       "      <th>feat_44</th>\n",
       "      <th>feat_45</th>\n",
       "      <th>feat_46</th>\n",
       "      <th>feat_47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1113027</td>\n",
       "      <td>33</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1115313</td>\n",
       "      <td>147</td>\n",
       "      <td>111</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1115907</td>\n",
       "      <td>528</td>\n",
       "      <td>136</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1116299</td>\n",
       "      <td>55</td>\n",
       "      <td>21</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1120373</td>\n",
       "      <td>62</td>\n",
       "      <td>54</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   form_id  views  submissions  feat_01  feat_02  feat_03  feat_04  feat_05  \\\n",
       "0  1113027     33           27      0.0      0.0      0.0      0.0      0.0   \n",
       "1  1115313    147          111      0.0      2.0      0.0      0.0      0.0   \n",
       "2  1115907    528          136      0.0      1.0      0.0      0.0      1.0   \n",
       "3  1116299     55           21      0.0      2.0      0.0      0.0      0.0   \n",
       "4  1120373     62           54      0.0      0.0      0.0      0.0      1.0   \n",
       "\n",
       "   feat_06  feat_07  ...  feat_38  feat_39  feat_40  feat_41  feat_42  \\\n",
       "0      0.0      1.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "1      0.0      0.0  ...      0.0      0.0      0.0      0.0      0.0   \n",
       "2      0.0      6.0  ...      0.0      0.0      0.0      0.0      1.0   \n",
       "3      1.0      2.0  ...      0.0      0.0      0.0      1.0      0.0   \n",
       "4      0.0      4.0  ...      0.0      0.0      0.0      0.0      1.0   \n",
       "\n",
       "   feat_43  feat_44  feat_45  feat_46  feat_47  \n",
       "0      0.0      0.0      2.0      1.0      2.0  \n",
       "1      0.0      0.0      0.0      0.0      0.0  \n",
       "2      1.0      0.0      0.0      0.0     30.0  \n",
       "3      0.0      0.0      0.0      0.0      7.0  \n",
       "4      1.0      1.0     11.0      7.0     21.0  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the target column and remove unwanted columns from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['completion_ratio'] = df['submissions']/df['views']\n",
    "selected_columns = list(df.columns)\n",
    "selected_columns.pop(selected_columns.index('form_id'))\n",
    "selected_columns.pop(selected_columns.index('views'))\n",
    "selected_columns.pop(selected_columns.index('submissions'))\n",
    "df = df.loc[:,selected_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = 'completion_ratio'\n",
    "features = list(df.columns)\n",
    "_  = features.pop(features.index(target_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sample the dataset for the analysis to finish on time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dataset length 1031284\n",
      "sampled dataset length 412514\n"
     ]
    }
   ],
   "source": [
    "df_original = df.copy()\n",
    "print(\"original dataset length\",len(df_original))\n",
    "df = df_original.sample(frac=0.4)\n",
    "print(\"sampled dataset length\",len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((371262, 48), (41252, 48))"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df,\n",
    "    df['completion_ratio'],\n",
    "    test_size=0.1,\n",
    "    random_state=7,\n",
    ")  \n",
    "\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "    'X_train': X_train.copy(),\n",
    "    'X_test': X_test.copy(),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Continuous vs discrete variables\n",
    "\n",
    "Let's determine which are continuous and discrete variables in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found possible discrete var with misleading type feat_01 2\n",
      "found possible discrete var with misleading type feat_04 4\n",
      "found possible discrete var with misleading type feat_08 13\n",
      "found possible discrete var with misleading type feat_10 10\n",
      "found possible discrete var with misleading type feat_13 17\n",
      "found possible discrete var with misleading type feat_15 10\n",
      "found possible discrete var with misleading type feat_20 2\n",
      "found possible discrete var with misleading type feat_35 14\n",
      "found possible discrete var with misleading type feat_44 19\n",
      "\n",
      "discrete vars ['feat_01', 'feat_04', 'feat_08', 'feat_10', 'feat_13', 'feat_15', 'feat_20', 'feat_35', 'feat_44']\n",
      "\n",
      "continuous vars ['feat_02', 'feat_03', 'feat_05', 'feat_06', 'feat_07', 'feat_09', 'feat_11', 'feat_12', 'feat_14', 'feat_16', 'feat_17', 'feat_18', 'feat_19', 'feat_21', 'feat_22', 'feat_23', 'feat_24', 'feat_25', 'feat_26', 'feat_27', 'feat_28', 'feat_29', 'feat_30', 'feat_31', 'feat_32', 'feat_33', 'feat_34', 'feat_36', 'feat_37', 'feat_38', 'feat_39', 'feat_40', 'feat_41', 'feat_42', 'feat_43', 'feat_45', 'feat_46', 'feat_47', 'completion_ratio']\n"
     ]
    }
   ],
   "source": [
    "discrete_vars = []\n",
    "for var in df.columns:\n",
    "    #print(var,len(df[var].unique()))\n",
    "    if len(df[var].unique()) == len(df):\n",
    "        print(\"found id or continuous var\",var)\n",
    "    elif df[var].dtypes == 'int64' and len(df[var].unique()) < 20:\n",
    "        print(\"found possible discrete var\",var)\n",
    "        discrete_vars.append(var)\n",
    "    elif len(df[var].unique()) < 20:\n",
    "        print(\"found possible discrete var with misleading type\",var, len(df[var].unique()))\n",
    "        discrete_vars.append(var)\n",
    "        \n",
    "continuous_vars = [ col for col in df.columns if col not in discrete_vars]\n",
    "print(\"\\ndiscrete vars\", discrete_vars)\n",
    "print(\"\\ncontinuous vars\", continuous_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Discrete variables reordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X_train', 'X_test', 'X_train_dr', 'X_test_dr'])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def assign_encoding_id(variable, x, encoding_dict):\n",
    "    try:\n",
    "        return encoding_dict[variable][x]\n",
    "    except:\n",
    "        encoding_dict[variable][x] = list(encoding_dict[variable].keys())[-1]+1\n",
    "        return encoding_dict[variable][x]\n",
    "    \n",
    "def discrete_reordering(X_train,X_test, discrete_vars ):\n",
    "    encoding_dict = {}\n",
    "    for variable in discrete_vars:    \n",
    "        # if we fit only with training data, then the splits must be \n",
    "        # stratified to each of the discrete vars that we are going\n",
    "        # to reorder! But this seems not possible as stratify does it by target classe only\n",
    "        # we will probably leak info into the model \n",
    "        # or do some other stuff with labels non seen like assing a new id on top of the rest\n",
    "        ordered_labels = X_train.groupby([\n",
    "                variable])['completion_ratio'].median().sort_values().index\n",
    "        ordinal_labels = {k: i for i, k in enumerate(ordered_labels, 0)}\n",
    "        # save the encoding dict\n",
    "        encoding_dict[variable] = ordinal_labels\n",
    "        X_train[variable] = X_train[variable].apply(lambda x: encoding_dict[variable][x])\n",
    "        X_test[variable] = X_test[variable].apply(lambda x: assign_encoding_id(variable,x, encoding_dict) )\n",
    "    return X_train, X_test\n",
    "\n",
    "new_modification_name = 'dr'\n",
    "old_keys = list(datasets.keys())\n",
    "for k in old_keys:\n",
    "    if k.find('train')>-1:\n",
    "        k_old_train = k\n",
    "        k_old_test = k.replace('train','test')\n",
    "        k_train = k+'_dr'\n",
    "        k_test = k_train.replace('train','test')\n",
    "        datasets[k_train], datasets[k_test] = discrete_reordering(datasets[k_old_train].copy(), datasets[k_old_test].copy(), discrete_vars)\n",
    "\n",
    "datasets.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Target transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_qt\n",
      "_dr_qt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['X_train', 'X_test', 'X_train_dr', 'X_test_dr', 'X_train_qt', 'X_test_qt', 'X_train_dr_qt', 'X_test_dr_qt'])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# function to transform train and test\n",
    "# pickle fitter quantile_transformer with suffix\n",
    "\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "def quantile_transformation(X_train, X_test, suffix, target_name):\n",
    "    quantile_transformer = preprocessing.QuantileTransformer(\n",
    "        output_distribution='normal', random_state=0)\n",
    "    \n",
    "    X_train[target_name] = quantile_transformer.fit_transform(X_train[target_name].to_numpy().reshape(-1,1))\n",
    "    X_test[target_name] = quantile_transformer.transform(X_test[target_name].to_numpy().reshape(-1,1))\n",
    "\n",
    "    pickle.dump(quantile_transformer, open('../data/qtransformer'+suffix+'.pkl','wb'))\n",
    "    return X_train, X_test\n",
    "\n",
    "new_modification_name = 'qt'\n",
    "old_keys = list(datasets.keys())\n",
    "for k in old_keys:\n",
    "    if k.find('train')>-1:\n",
    "        k_old_train = k\n",
    "        k_old_test = k.replace('train','test')\n",
    "        k_train = k+'_'+new_modification_name\n",
    "        k_test = k_train.replace('train','test')\n",
    "        suffix = k_train[k_train.find('_')+6:]\n",
    "        print(suffix)\n",
    "        datasets[k_train], datasets[k_test] = quantile_transformation(\n",
    "            datasets[k_old_train].copy(), datasets[k_old_test].copy(), suffix, 'completion_ratio')\n",
    "\n",
    "datasets.keys()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Correlation\n",
    "\n",
    "Let's detect if there are correlated features that can harm the machine learning model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.set(style=\"white\")\n",
    "\n",
    "\n",
    "d = X_train\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = d.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlation between features\n",
      "Top Absolute Correlations\n",
      "feat_06  feat_34    1.000000\n",
      "feat_36  feat_38    0.999088\n",
      "feat_18  feat_39    0.930603\n",
      "feat_19  feat_38    0.905821\n",
      "         feat_36    0.905540\n",
      "feat_42  feat_43    0.887645\n",
      "feat_19  feat_40    0.839575\n",
      "feat_38  feat_40    0.825255\n",
      "feat_36  feat_40    0.824710\n",
      "feat_19  feat_33    0.819799\n",
      "feat_07  feat_47    0.818301\n",
      "         feat_30    0.784156\n",
      "feat_45  feat_46    0.767886\n",
      "feat_05  feat_43    0.767480\n",
      "feat_33  feat_38    0.742714\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Correlation between features\")\n",
    "corr = d.corr()\n",
    "# construct pairs (corr, (feat, feat)) and then sort them\n",
    "def get_redundant_pairs(df):\n",
    "    '''Get diagonal and lower triangular pairs of correlation matrix'''\n",
    "    pairs_to_drop = set()\n",
    "    cols = df.columns\n",
    "    for i in range(0, df.shape[1]):\n",
    "        for j in range(0, i+1):\n",
    "            pairs_to_drop.add((cols[i], cols[j]))\n",
    "    return pairs_to_drop\n",
    "\n",
    "def get_top_abs_correlations(df, n=5):\n",
    "    au_corr = df.corr().abs().unstack()\n",
    "    labels_to_drop = get_redundant_pairs(df)\n",
    "    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n",
    "    return au_corr[0:n]\n",
    "\n",
    "print(\"Top Absolute Correlations\")\n",
    "print(get_top_abs_correlations(X_train, 15))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlated_features = [\n",
    "    'feat_06',\n",
    "    'feat_38', # 36, 19, 40, 33\n",
    "    'feat_36', # 38, 19, 40\n",
    "    'feat_19', # 38, 36, 40, 33\n",
    "    #'feat_40', # 19, 38, 36 -> don't removve\n",
    "    'feat_43',\n",
    "    'feat_07', # 47, 30\n",
    "    'feat_43', # 42, 05\n",
    "    'feat_46'\n",
    "]\n",
    "non_correlated_vars = [col for col in df.columns if col not in correlated_features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def setup_correlated_vars(X_train, X_test,non_correlated_vars):\n",
    "    X_train = X_train.loc[:,non_correlated_vars].copy()\n",
    "    X_test = X_test.loc[:,non_correlated_vars].copy()\n",
    "    return X_train, X_test\n",
    "\n",
    "new_modification_name = 'nc'\n",
    "old_keys = list(datasets.keys())\n",
    "for k in old_keys:\n",
    "    if k.find('train')>-1:\n",
    "        k_old_train = k\n",
    "        k_old_test = k.replace('train','test')\n",
    "        k_train = k+'_'+new_modification_name\n",
    "        k_test = k_train.replace('train','test')\n",
    "        datasets[k_train], datasets[k_test] = setup_correlated_vars(\n",
    "            datasets[k_old_train].copy(), datasets[k_old_test].copy(), non_correlated_vars)\n",
    "\n",
    "datasets.keys()\n",
    "len(datasets.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.         0.         ... 0.         0.00584795 0.535918  ]\n",
      " [0.         0.         0.         ... 0.         0.00584795 0.45228417]\n",
      " [0.         0.03921569 0.         ... 0.         0.         0.37488955]\n",
      " ...\n",
      " [1.         0.         0.03333333 ... 0.         0.         0.08025707]\n",
      " [1.         0.         0.         ... 0.         0.         0.95917646]\n",
      " [0.         0.05882353 0.         ... 0.06015038 0.02046784 0.55924415]]\n",
      "[[0.         0.         0.         ... 0.         0.00584795 0.535918  ]\n",
      " [1.         0.         0.         ... 0.         0.00584795 0.45228417]\n",
      " [1.         0.03921569 0.         ... 0.         0.         0.37488955]\n",
      " ...\n",
      " [0.         0.         0.03333333 ... 0.         0.         0.08025707]\n",
      " [0.         0.         0.         ... 0.         0.         0.95917646]\n",
      " [1.         0.05882353 0.         ... 0.06015038 0.02046784 0.55924415]]\n",
      "[[1.         0.         0.         ... 0.         0.00584795 0.50188897]\n",
      " [0.         0.         0.         ... 0.         0.00584795 0.47032704]\n",
      " [0.         0.03921569 0.         ... 0.         0.         0.44302701]\n",
      " ...\n",
      " [1.         0.         0.03333333 ... 0.         0.         0.31392189]\n",
      " [1.         0.         0.         ... 0.         0.         0.72813416]\n",
      " [0.         0.05882353 0.         ... 0.06015038 0.02046784 0.51129722]]\n",
      "[[0.         0.         0.         ... 0.         0.00584795 0.50188897]\n",
      " [1.         0.         0.         ... 0.         0.00584795 0.47032704]\n",
      " [1.         0.03921569 0.         ... 0.         0.         0.44302701]\n",
      " ...\n",
      " [0.         0.         0.03333333 ... 0.         0.         0.31392189]\n",
      " [0.         0.         0.         ... 0.         0.         0.72813416]\n",
      " [1.         0.05882353 0.         ... 0.06015038 0.02046784 0.51129722]]\n",
      "[[1.         0.         0.         ... 0.         0.00584795 0.535918  ]\n",
      " [0.         0.         0.         ... 0.00322581 0.00584795 0.45228417]\n",
      " [0.         0.03921569 0.         ... 0.00322581 0.         0.37488955]\n",
      " ...\n",
      " [1.         0.         0.03333333 ... 0.         0.         0.08025707]\n",
      " [1.         0.         0.         ... 0.         0.         0.95917646]\n",
      " [0.         0.05882353 0.         ... 0.01935484 0.02046784 0.55924415]]\n",
      "[[0.         0.         0.         ... 0.         0.00584795 0.535918  ]\n",
      " [1.         0.         0.         ... 0.00322581 0.00584795 0.45228417]\n",
      " [1.         0.03921569 0.         ... 0.00322581 0.         0.37488955]\n",
      " ...\n",
      " [0.         0.         0.03333333 ... 0.         0.         0.08025707]\n",
      " [0.         0.         0.         ... 0.         0.         0.95917646]\n",
      " [1.         0.05882353 0.         ... 0.01935484 0.02046784 0.55924415]]\n",
      "[[1.         0.         0.         ... 0.         0.00584795 0.50188897]\n",
      " [0.         0.         0.         ... 0.00322581 0.00584795 0.47032704]\n",
      " [0.         0.03921569 0.         ... 0.00322581 0.         0.44302701]\n",
      " ...\n",
      " [1.         0.         0.03333333 ... 0.         0.         0.31392189]\n",
      " [1.         0.         0.         ... 0.         0.         0.72813416]\n",
      " [0.         0.05882353 0.         ... 0.01935484 0.02046784 0.51129722]]\n",
      "[[0.         0.         0.         ... 0.         0.00584795 0.50188897]\n",
      " [1.         0.         0.         ... 0.00322581 0.00584795 0.47032704]\n",
      " [1.         0.03921569 0.         ... 0.00322581 0.         0.44302701]\n",
      " ...\n",
      " [0.         0.         0.03333333 ... 0.         0.         0.31392189]\n",
      " [0.         0.         0.         ... 0.         0.         0.72813416]\n",
      " [1.         0.05882353 0.         ... 0.01935484 0.02046784 0.51129722]]\n",
      "dict_keys(['X_train', 'X_test', 'X_train_dr', 'X_test_dr', 'X_train_qt', 'X_test_qt', 'X_train_dr_qt', 'X_test_dr_qt', 'X_train_nc', 'X_test_nc', 'X_train_dr_nc', 'X_test_dr_nc', 'X_train_qt_nc', 'X_test_qt_nc', 'X_train_dr_qt_nc', 'X_test_dr_qt_nc', 'X_train_mmxs', 'X_test_mmxs', 'X_train_dr_mmxs', 'X_test_dr_mmxs', 'X_train_qt_mmxs', 'X_test_qt_mmxs', 'X_train_dr_qt_mmxs', 'X_test_dr_qt_mmxs', 'X_train_nc_mmxs', 'X_test_nc_mmxs', 'X_train_dr_nc_mmxs', 'X_test_dr_nc_mmxs', 'X_train_qt_nc_mmxs', 'X_test_qt_nc_mmxs', 'X_train_dr_qt_nc_mmxs', 'X_test_dr_qt_nc_mmxs']) 32\n"
     ]
    }
   ],
   "source": [
    "def scaler_minmax(X_train, X_test):\n",
    "    scaler = MinMaxScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return pd.DataFrame(X_train), pd.DataFrame(X_test)\n",
    "\n",
    "new_modification_name = 'mmxs'\n",
    "old_keys = list(datasets.keys())\n",
    "for k in old_keys:\n",
    "    if k.find('train')>-1:\n",
    "        k_old_train = k\n",
    "        k_old_test = k.replace('train','test')\n",
    "        k_train = k+'_'+new_modification_name\n",
    "        k_test = k_train.replace('train','test')\n",
    "        datasets[k_train], datasets[k_test] = scaler_minmax(\n",
    "            datasets[k_old_train].copy(), \n",
    "            datasets[k_old_test].copy())\n",
    "\n",
    "print(datasets.keys(),len(datasets.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\n",
      "selected features: 41\n",
      "X_train_dr\n",
      "selected features: 41\n",
      "X_train_qt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/disk/home/pau/Projectes/typeform_test/venv/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 17095.377243188734, tolerance: 40.626163069940084\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected features: 45\n",
      "X_train_dr_qt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/disk/home/pau/Projectes/typeform_test/venv/lib/python3.7/site-packages/sklearn/linear_model/_coordinate_descent.py:531: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 17095.377243188734, tolerance: 40.626163069940084\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "selected features: 45\n",
      "X_train_nc\n",
      "selected features: 39\n",
      "X_train_dr_nc\n",
      "selected features: 39\n",
      "X_train_qt_nc\n",
      "selected features: 40\n",
      "X_train_dr_qt_nc\n",
      "selected features: 40\n",
      "dict_keys(['X_train', 'X_test', 'X_train_dr', 'X_test_dr', 'X_train_qt', 'X_test_qt', 'X_train_dr_qt', 'X_test_dr_qt', 'X_train_nc', 'X_test_nc', 'X_train_dr_nc', 'X_test_dr_nc', 'X_train_qt_nc', 'X_test_qt_nc', 'X_train_dr_qt_nc', 'X_test_dr_qt_nc', 'X_train_fs', 'X_test_fs', 'X_train_dr_fs', 'X_test_dr_fs', 'X_train_qt_fs', 'X_test_qt_fs', 'X_train_dr_qt_fs', 'X_test_dr_qt_fs', 'X_train_nc_fs', 'X_test_nc_fs', 'X_train_dr_nc_fs', 'X_test_dr_nc_fs', 'X_train_qt_nc_fs', 'X_test_qt_nc_fs', 'X_train_dr_qt_nc_fs', 'X_test_dr_qt_nc_fs']) 32\n"
     ]
    }
   ],
   "source": [
    "def feature_selection(X_train, X_test):\n",
    "    sel_ = SelectFromModel(Lasso(alpha=0.00005, random_state=7))\n",
    "\n",
    "    y_train = X_train.loc[:,'completion_ratio'].to_numpy()\n",
    "    y_test = X_test.loc[:,'completion_ratio'].to_numpy()\n",
    "    train_vars = [col for col in X_train.columns if col != 'completion_ratio']\n",
    "    \n",
    "    # train Lasso model and select features\n",
    "    X_train = X_train.loc[:,train_vars].copy()\n",
    "    sel_.fit(X_train, y_train)\n",
    "    selected_feats = X_train.columns[(sel_.get_support())]\n",
    "    \n",
    "    X_train = X_train.loc[:,selected_feats].copy()\n",
    "    X_test = X_test.loc[:,selected_feats].copy()\n",
    "\n",
    "    # let's print some stats\n",
    "    #print('total features: {}'.format((X_train_ls.shape[1])))\n",
    "    print('selected features: {}'.format(len(selected_feats)))\n",
    "    #print('features with coefficients shrank to zero: {}'.format(\n",
    "    #    np.sum(sel_.estimator_.coef_ == 0)))\n",
    "    #print(selected_feats)\n",
    "    #pd.Series(selected_feats).to_csv('../data/selected_features.csv', index=False)\n",
    "\n",
    "    X_train['completion_ratio'] = y_train\n",
    "    X_test['completion_ratio'] = y_test\n",
    "    return X_train, X_test\n",
    "\n",
    "new_modification_name = 'fs'\n",
    "old_keys = list(datasets.keys())\n",
    "for k in old_keys:\n",
    "    if k.find('train')>-1:\n",
    "        k_old_train = k\n",
    "        k_old_test = k.replace('train','test')\n",
    "        k_train = k+'_'+new_modification_name\n",
    "        k_test = k_train.replace('train','test')\n",
    "        print(k_old_train)\n",
    "        datasets[k_train], datasets[k_test] = feature_selection(\n",
    "            datasets[k_old_train].copy(), \n",
    "            datasets[k_old_test].copy())\n",
    "\n",
    "print(datasets.keys(),len(datasets.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Saving processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = list(datasets.keys())\n",
    "pickle.dump(models_list, open('../data/models_list.pkl','wb'))\n",
    "for k,v in datasets.items():\n",
    "    v.to_csv('../data/'+k+'.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "typeform_test",
   "language": "python",
   "name": "typeform_test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
